<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Past Projects | Bingjun Guo</title>
    <link rel="stylesheet" href="my_stylesheet.css">
</head>

<body>

<header>
    <nav class="subpages">
        <ul>
            <li class="main"><a href = "index.html">Bingjun Guo</a></li>            
            <li class="section"><a href = "acknowledgements.html">Acks</a></li>
            <li class="section"><a href = "babblings.html">Babblings</a></li>
            <li class="section"><a href = "courses.html">Courses</a></li>
        </ul>
    </nav>
</header>

<main>
<div class="wrapper">
    <div class="intro">
        <h1>Past Projects</h1>
        <p>Here are the projects that I have worked on. Almost all of them were accomplished in cooperation with my wonderful classmates. Since most of them seem a bit weird now, this section will be updated slower.</p>
    </div>
    <div class="project">
        <h2>A Unix-like Operating System</h2>
        <div class="info">
            Course Project, Fall 2023 (junior year)
        </div>
        <p>
            It's gonna be sure that almost every one of us (who didn't fail the course) would have this project in resume as one of the biggest bosses from our courses, which marks almost an end for our journey from hardware to software. Basically in group of four, we spent half a semester on building the kernel of this mini multitask terminal-interactive x86-arch operating system. I was mainly responsible for building terminals and assembly linkages, which required sufficient familiarity with all features of our operating system, including interrupts, scheduling, virtual memory, and file system. From my perspective of view, an operating system could be all about synchronization and interaction, which were equally important when switching among multiple terminals. As supplementary components, we added support for command history, auto command completion, terminal color switch, and a mouse cursor right in our kernel. Frankly speaking these were likely to be just above normal, as there were some groups doing seriously marvelous jobs.</p>
    </div>
    <div class="project">
        <h2>A LLM Augmented Data Cleaning Framework</h2>
        <div class="info">
            Summer Research Project, Summer 2023 (sophomore year)
        </div>
        <p>
            When this project was handed us, it was just a few keywords, and we were just with a few blank minds. We got familiar with some data error detecting and correlation methods in the first week, yet none of us had experience with any language models. After the first discussion (or the second), the other two group members went on their journey on building a data cleaning pipeline in classic style, while I set off for blending the LLM or pre-trained LMs with the pipeline. <br>
            <br>In the following week, I finished my exploration from RNNs to transformer from scratch and started to seriously consider on the two papers that mentors offered us. One was a data representation method utilizing contrastive pretraining and transformer-based structure, and the other was an experiment that succeeded on predicting time-series data with semi-frozen GPT-2 parameters. To be honest I really wished to do something fancier at first, but both time and resources was rather limited then. Hence, after getting familiar with the codes, what I did was just basically plugging the BERT parameters in the representation framework for tabular data, and praying for a better performance on representing dirty data. <br><br>
            Then I did quite a number of experiments on data with different features and different frozen conditions. It was found that the finetuning merely on norm layers barely changed anything with our training setting, and the full parameter fine-tuning appeared easy to overfit on most datasets, which were limited either in features or in samples. Fortunately we did find something seemed better, when applied this approach on samples with sufficiently large amount of both features (50~200) and samples. The reason why it only "seemed" so was that the finetuning on those data took incredibly long on single A40, and there was barely any more time to test on the original model with same or larger amount of parameters. Thus, we just left the job to explain these things in our report and headed for our new semester which was rather vital and challenging.
        </p>
    </div>
    <div class="project">
        <h2></h2>
        <p>
            
        </p>
    </div>
    <div class="project">
        <a href="secrets/ECE_110_final_report.pdf"><h2>A Processor-Free Intelligent Car</h2></a>
        <div class="info">
            Course Project, Fall 2021 (freshman year)
        </div>
        <p>
            This is a car that marches following a black path on its own, keeping a distance from obstacles in front of it utilizing an ultrasonic sensor. Where we were quite proud of it was that instead of relying on programmed Arduino, such functions were realized with logic circuits merely consisting of diodes, resistors, and MOSFETs. We made the car processed signals and applied control completely at hardware level. It was said that we were the only group achieving so.
        </p>
    </div>
</div>
</main>
<footer>
    <p>
        &copy; 2024 Bingjun Guo
    </p>
</footer>

</body>

</html>